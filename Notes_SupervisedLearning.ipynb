{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1\n",
    "\n",
    "# Types of Machine Learning\n",
    "\n",
    "## Supervised Learning\n",
    "1. Classification: predict categorical Outcomes\n",
    "2. Regression: predict numeric outcomes\n",
    "\n",
    "## Unsupervised Learning (No correct labels)\n",
    "\n",
    "Reduce data set to a set of features\n",
    "\n",
    "## Reinforcement Learning (Receive reward and make decision)\n",
    "\n",
    "# Deep Learning\n",
    "It has 3 barries:\n",
    "1. Large data set\n",
    "2. Higher computing power\n",
    "3. Hard to understand why certain decisions are made\n",
    "\n",
    "# Machine Learning Libraries\n",
    "\n",
    "1. TensorFlow\n",
    "2. Sickit-learn\n",
    "\n",
    "# Ethics in Machine Learning\n",
    "The potential bias built in the data should be always kept in mind.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Linear Regression\n",
    "\n",
    "## Absolute Trick\n",
    "\n",
    "Given a data point $(p,q)$ and a line $$y = w_1x + w_2.$$\n",
    "Linear Regression moves the line closer to the data point.\n",
    "\n",
    "Absolute Trick is : $$y = (w_1 \\pm p \\alpha) x + (w_2 \\pm \\alpha),$$ which will slightly move the line and $\\alpha$ is called the *learning rate*.\n",
    "\n",
    "add when the point is above the line.\n",
    "### Why **p** is there?\n",
    "1. It affects the direction of the rotation\n",
    "2. It affects the scale of the rotation and moving\n",
    "\n",
    "## Square trick\n",
    "\n",
    "consider the distance between the point and the line\n",
    "\n",
    "$$\n",
    "y = w_1x + w_2\n",
    "$$\n",
    "q' = w1p + w_2\n",
    "\n",
    "$$\n",
    "y = (w_1 + p(q-q')\\alpha)x + (w_2 + (q-q')\\alpha)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Move the line to mimize the error\n",
    "\n",
    "$$\n",
    "w_i \\rightarrow w_i -\\alpha \\frac{\\partial Error}{\\partial w_i}\n",
    "$$\n",
    "\n",
    "## Error Function\n",
    "### Mean Absolute Error\n",
    "\n",
    "$$\n",
    "MAE = \\frac{1}{m} \\sum_{i = 1}^m |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "### Mean Squared Error\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{2m} \\sum_{i = 1}^m (y_i  - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Note that $1/2$ helps when taking derivative.\n",
    "\n",
    "    **Gradient Descent with MSE function (with 1 data point) is equivalent to the Square Trick**\n",
    "    **Gradient Descent with MAE function (with 1 data point) is equivalent to the Absolute Trick **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting a random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# TODO: Fill in code in the function below to implement a gradient descent\n",
    "# step for linear regression, following a squared error rule. See the docstring\n",
    "# for parameters and returned variables.\n",
    "def MSEStep(X, y, W, b, learn_rate = 0.005):\n",
    "    \"\"\"\n",
    "    This function implements the gradient descent step for squared error as a\n",
    "    performance metric.\n",
    "    \n",
    "    Parameters\n",
    "    X : array of predictor features\n",
    "    y : array of outcome values\n",
    "    W : predictor feature coefficients\n",
    "    b : regression function intercept\n",
    "    learn_rate : learning rate\n",
    "\n",
    "    Returns\n",
    "    W_new : predictor feature coefficients following gradient descent step\n",
    "    b_new : intercept following gradient descent step\n",
    "    \"\"\"\n",
    "    \n",
    "    y_hat = np.matmul(X, W) + b\n",
    "    W_new =  W + learn_rate * 1/len(y) *sum(np.matmul((y-y_hat), X))\n",
    "    b_new = b + learn_rate * 1/len(y) * sum(y- y_hat)\n",
    "    return W_new, b_new\n",
    "\n",
    "\n",
    "# The parts of the script below will be run when you press the \"Test Run\"\n",
    "# button. The gradient descent step will be performed multiple times on\n",
    "# the provided dataset, and the returned list of regression coefficients\n",
    "# will be plotted.\n",
    "def miniBatchGD(X, y, batch_size = 20, learn_rate = 0.1, num_iter = 25):\n",
    "    \"\"\"\n",
    "    This function performs mini-batch gradient descent on a given dataset.\n",
    "\n",
    "    Parameters\n",
    "    X : array of predictor features\n",
    "    y : array of outcome values\n",
    "    batch_size : how many data points will be sampled for each iteration\n",
    "    learn_rate : learning rate\n",
    "    num_iter : number of batches used\n",
    "\n",
    "    Returns\n",
    "    regression_coef : array of slopes and intercepts generated by gradient\n",
    "      descent procedure\n",
    "    \"\"\"\n",
    "    n_points = X.shape[0]\n",
    "    W = np.zeros(X.shape[1]) # coefficients\n",
    "    b = 0 # intercept\n",
    "    \n",
    "    # run iterations\n",
    "    regression_coef = [np.hstack((W,b))]\n",
    "    for _ in range(num_iter):\n",
    "        batch = np.random.choice(range(n_points), batch_size)\n",
    "        X_batch = X[batch,:]\n",
    "        y_batch = y[batch]\n",
    "        W, b = MSEStep(X_batch, y_batch, W, b, learn_rate)\n",
    "        regression_coef.append(np.hstack((W,b)))\n",
    "    \n",
    "    return regression_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE or MSE ??\n",
    "### MAE returns multiple parallel lines which give the same MAE\n",
    "### MSE returns a unique line, why?\n",
    "    MSE is a quadratic function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher Dimension --> Multiple linear regression\n",
    "\n",
    "# Closed Form Solution\n",
    "    MSE has explicit solution, however using it may involve inverting high dimensional matrix, which is computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Warnings\n",
    "\n",
    "**Linear Regression Works Best When the Data is Linear**\n",
    "\n",
    "Linear regression produces a straight line model from the training data. If the relationship in the training data is not really linear, you'll need to either make adjustments (transform your training data), add features (we'll come to this next), or use another kind of model.\n",
    "\n",
    "**Linear Regression is Sensitive to Outliers**\n",
    "\n",
    "Linear regression tries to find a 'best fit' line among the training data. If your dataset has some outlying extreme values that don't fit a general pattern, they can have a surprisingly large effect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "**Adding power terms**\n",
    "\n",
    "# Regulation (for Regression and Classification)\n",
    "\n",
    "## L1 regulation\n",
    "\n",
    "    Add errors by sum of the absolute value of the coefficients and multiply by the tuning parameter $\\lambda$.\n",
    "    \n",
    "    1. Computationally inefficient\n",
    "    2. Sparse Outputs\n",
    "    3. Feature selection\n",
    "## L2 regulation\n",
    "    Add l2 norm\n",
    "    \n",
    "    1. Computationally efficient\n",
    "    2. Non-sparse Outputs\n",
    "    3. No-feature selection\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling\n",
    "1. Standardizing mean 0 std 1\n",
    "2. Normalizing range 0,1\n",
    "\n",
    "## When to scaling\n",
    "1. Distance-based metric model: KNN, SVM\n",
    "2. Regularization: LASSO, RIDGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## June 10, 2020\n",
    "\n",
    "# Lesson 3 Perceptron Algorithm\n",
    "\n",
    "This lesson will discuss about the classification problem.\n",
    "E.g., Is this email spam or not?\n",
    "\n",
    "## Linear Bounaries\n",
    "\n",
    "1. Example: Acceptance at University\n",
    "\n",
    "    Boundary a line: \n",
    "$$\n",
    "Wx + b = 0, W = (w_1, w_2), x = (x_1, x_2)\n",
    "$$\n",
    "\n",
    "    y = label: 0 or 1\n",
    "    Prediction:\n",
    "$$\n",
    "\\hat{y} = \\begin{cases} 1 &if \\quad Wx + b \\geq 0\\\\ 0 &if \\quad Wx + b <0\\end{cases}\n",
    "$$\n",
    "\n",
    "## Higher Dimensions\n",
    "Features:\n",
    "n-dimensional space:\n",
    "$x_1, x_2, \\cdots, x_n$\n",
    "\n",
    "Dimension: (n-1) dimensional hyperplane\n",
    "\n",
    "$$\n",
    "w_1x_1 + w_2x_2 + \\cdots + w_nx_n + b = 0, \\text{ or } Wx + b  = 0\n",
    "$$\n",
    "\n",
    "## Perceptrons\n",
    "\n",
    "It is an encoding of building blocks in Neural network.\n",
    "\n",
    "Percetrons checks if the point is in the positive or negative area.\n",
    "\n",
    "1. End input nodes: $x1, x_2, bias = 1$\n",
    "\n",
    "    Note the bias can be put in the input nodes or the inside the first node\n",
    "\n",
    "2. Edges with Weigths: $w_1, w_2, b$\n",
    "\n",
    "3. First Node: Linear funciton to calculate the linear boundary\n",
    "\n",
    "4. Second Node: Step function to calculate the 0 or 1\n",
    "\n",
    "## Perceptrons as Logical Operators\n",
    "\n",
    "### AND/OR/NOT Perceptron: \n",
    "1. Turn True/False to 1/0 \n",
    "2. Add weight and bias\n",
    "3. Compute the linear function\n",
    "4. Output True or False\n",
    "\n",
    "### XOR Perceptron: return 1 exactly one element is True\n",
    "XOR Neural network\n",
    "\n",
    "## Perceptron Trick\n",
    "\n",
    "It is hard to build perceptrons in real life unlike the simple basic operators.\n",
    "\n",
    "### Goal: Split Data\n",
    "\n",
    "1. Define a linear boundary\n",
    "2. Update the line\n",
    "    The misclassified point want the line to be closer \n",
    "    Given a misclassified point(p1, p2), provide a learning rate $\\alpha$.\n",
    "    - When the point is in the positive area\n",
    "$$\n",
    "(w_1 - \\alpha * p_1) x_1 + (w_2 - \\alpha * p_2)x_2   + (b-\\alpha*1) = 0\n",
    "$$\n",
    "\n",
    "    - When the point is in the negative area\n",
    "$$\n",
    "(w_1  + \\alpha * p_1) x_1 + (w_2 + \\alpha * p_2)x_2   + (b+\\alpha*1) = 0\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Algorithm\n",
    "Repeat the perceptron trick for all misclassified points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4\n",
    "\n",
    "# Decision Tree\n",
    "\n",
    "## Recommding Apps\n",
    "\n",
    "**Our task is to recommend the apps the customers most likely to download**\n",
    "\n",
    "## Entropy\n",
    "\n",
    "Recall its definition in physics: ice < water < gas\n",
    "\n",
    "### Entropy in probability\n",
    "4 red balls < 3 red balls  + 1 blue < 2 red + 2 blue\n",
    "knowledge: > > \n",
    "\n",
    "  **Entropy and knowledge are opposite**\n",
    "    \n",
    "### Entropy formula\n",
    "\n",
    "Entropy is equal to the sum of negative logarithm probability of winning in each draw divided by the number of draws.\n",
    "\n",
    "\n",
    "Given $m$ red balls and $n$ blue balls, \n",
    "$$\n",
    "Entropy = - \\frac{m}{m+n} \\log_2(\\frac{m}{m+n}) - \\frac{n}{m+n} \\log_2(\\frac{ n}{m+n})\n",
    "$$\n",
    "\n",
    "### Multi-class Entropy\n",
    "\n",
    "$$\n",
    "entropy = -\\sum_{i = 1}^n p_i * \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "1. Minimum entropy = 0 when all elements belong to one class\n",
    "\n",
    "2. Maximum entropy is achieved when all classes have the same probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "\n",
    "Information Gain = Entropy(Parent) - sum ( proportion of the child  * the entropy(child))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximizing Informaion gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the recommmendation app problem\n",
    "\n",
    "**Calculate the entropy of the column of the labels, pick the label with the largest information gain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>App</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>Study</td>\n",
       "      <td>Pokemon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>Work</td>\n",
       "      <td>WhatsApp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>Work</td>\n",
       "      <td>SnaptChat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>Work</td>\n",
       "      <td>WhatsApp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>Study</td>\n",
       "      <td>Pokemon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>Study</td>\n",
       "      <td>Pokemon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gender Occupation        App\n",
       "0      F      Study    Pokemon\n",
       "1      F       Work   WhatsApp\n",
       "2      M       Work  SnaptChat\n",
       "3      F       Work   WhatsApp\n",
       "4      M      Study    Pokemon\n",
       "5      M      Study    Pokemon"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {'Gender':['F', 'F', 'M', 'F', 'M', 'M'], \\\n",
    "        'Occupation': ['Study', 'Work', 'Work', 'Work', 'Study', 'Study'],\\\n",
    "        'App': ['Pokemon', 'WhatsApp', 'SnaptChat', 'WhatsApp', 'Pokemon', 'Pokemon']}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. App: $-3/6 \\log_2(3/6 ) -2/6 * \\log_2(2/6) - 1/6 \\log_2(1/6) = 1.46 = $\n",
    "2. Gender: $0.92$--> information gain is $1.46 - 0.92 = 0.54$\n",
    "3. Occupation: $0.46$ --> information gain is $1.0$\n",
    "\n",
    "Thus, occupation is the best label for classification.\n",
    "\n",
    "Decision tree will do the following\n",
    "\n",
    "Given a person with gender and occupation:\n",
    "First check occupation then check gender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximizing Infromation gains with continuous variables\n",
    "\n",
    "Repeat the cuts in each dimension and construct the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def entropy(c1, c2):\n",
    "   return  -c1/(c1+c2)* np.log2(c1/(c1+c2)) - c2/(c1+c2)*np.log2(c2/(c1+c2))\n",
    "\n",
    "def info_gain(g1, g2, c1, c2, c3, c4):\n",
    "    return entropy(g1, g2) - (c1+c2)/(g1+g2) * entropy(c1, c2) - (c3+c4)/(g1+g2)* entropy(c3, c4)\n",
    "    \n",
    "data = pd.read_csv('ml-bugs.csv')\n",
    "g1 = 10\n",
    "g2 = 14\n",
    "\n",
    "group = data.groupby('less20')['Species'].value_counts()\n",
    "c1 = group[(True, 'Mobug')]\n",
    "c2 = group[(True, 'Lobug')]\n",
    "c3 = group[(False, 'Mobug')] \n",
    "c4 = group[(False, 'Lobug')] \n",
    "print(info_gain(g1,g2,c1,c2,c3,c4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters for Decision Trees\n",
    "\n",
    "1. **Maximum Depth**\n",
    "    A tree of maximum length *k* can have at most $2^k$ leaves.\n",
    "2. **Minimum number of samples to split**\n",
    "    If a node has fewer samples than pre-defined size, it will not be split and the splitting process stops\n",
    "3. **Minimum number of samples per leaf**\n",
    "    Integer: minimum number of samples\n",
    "    float: percentage of samples\n",
    "    If it is violated then the split is not allowed\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees in sklearn\n",
    "\n",
    "For your decision tree model, you will be using sk-learn *Decision Tree Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    model = DecisionTreeClassifier( here you can put the hyperparameters)\n",
    "\n",
    "    model.fit(x_values, y_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    max_depth\n",
    "\n",
    "    min_samples_leaf\n",
    "\n",
    "    min_samples_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4 Naive Bayes\n",
    "\n",
    "100 sick patient diagnosis 99 \n",
    "100 healthy patient diagnosis 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_posi = 0.99 /10000 + 0.01 *9999/10000 \n",
    "p_sick_posi = 0.99 /10000\n",
    "posterior = p_sick_posi/p_posi\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Positives\n",
    "\n",
    "1% error means 1 out of 100 is wrongly marked as positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Algorithm\n",
    "\n",
    "### Naive Assumption\n",
    "Not affect much on the results, but makes the calculation easier\n",
    "$$\n",
    "p(AB)  = P(A) P(B)\n",
    "$$\n",
    "\n",
    "### Conditional Probability\n",
    "$$\n",
    "p(A|B)P(B) = P(B|A)P(A)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(A|B)\\prop  P(B|A)P(A)\n",
    "$$\n",
    "\n",
    "## Compute the exact posterior probability\n",
    "\n",
    "Normalize the values computed according to Bayes rule\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to do the Naive Bayes Algorithm\n",
    "1. Exchange the order of the events\n",
    "2. Compute the conditional probability based on the data and the Naive Assumption\n",
    "3. Normalize the probabilities to get the exact values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6: Support Vector Machines\n",
    "## Maximize the margin: \n",
    "\n",
    "Goal, we want the boundary separate the data points and far away from the points as far as possible\n",
    "\n",
    "**Error**: Classifiation error plus margin error\n",
    "\n",
    "### Classification Error\n",
    "1. Boundary: $Wx + b = 0$\n",
    "2. Margins: $Wx + b = 1, Wx+b = -1$ *we don't want any points classified in the margins*\n",
    "\n",
    "3. Classification error: the distance between the point and the corresponding margins.\n",
    "\n",
    "### Margin Error\n",
    "\n",
    "A function gives large error for small margin\n",
    "\n",
    "\n",
    "**$$\n",
    "Margin = \\frac{2}{|W|} \\rightarrow Error = |W|^2\n",
    "$$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The C parameter\n",
    "\n",
    "**Error = C * Classification error  + Margin Error**\n",
    "\n",
    "1. Small C: Large margin, may make some classification error\n",
    "2. Large C: classifies points well, but may have a small margin\n",
    "\n",
    "*Note to find a good C, grid search will be used*\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Kernel\n",
    "## RBF Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from sklearn.svm import SVC\n",
    "\n",
    "## Hyper parameters\n",
    "1. The C parameter: balance between classification error and margin error\n",
    "\n",
    "2. kernel: 'linear', 'poly', 'rbf'\n",
    "\n",
    "3. degree: if the kernel is polynomial, it is the max degree\n",
    "\n",
    "4. gamma: if the kernel is rbf, it is the gamma parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 7 Ensemble Methods (BAGGING & BOOSTING)\n",
    "\n",
    "**Decision trees tend to overfit the data**\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "1. Pick subset of features randomly to implementing *decision tree*\n",
    "2. Given a new data point, let trees do the prediction\n",
    "3. Pick the choice with the highest probability\n",
    "\n",
    "## BAGGING\n",
    "\n",
    "1. Pick subset of the data and build learner 1\n",
    "2. Pick the second subset of the data and build the learner 2\n",
    "3. Do the prediciton on the whole data set and voting\n",
    "\n",
    "## ADABOOST\n",
    "\n",
    "1. 1st learner: Fit max accuaracy\n",
    "    Weight = 1; \n",
    "    correct: 7 , incorrect: 3\n",
    "    \n",
    "2. 2nd Learner: fix the misclassified points by punish more on the misclassified points.\n",
    "    Weight(misclassified) = 7/3 (take the model 50-50)\n",
    "    correct: 11, incorrect: 3\n",
    "    \n",
    "3. 3rd learner: enlarging the points and fix it\n",
    "    Weight(misclassified) = 11/3\n",
    "    correct: 19, incorrect = 3\n",
    "    \n",
    "4. Combine three learners\n",
    "    Assign large positive weight to the *truthful* model, zero weight to the random (useless) model and large negative weight to the *total liar* model.\n",
    "    \n",
    "    1. It is related to the accuracy: truthful (1.0), random(0.5), liar(0)\n",
    "    \n",
    "    2. $y = ln(\\frac{x}{1-x})$ helps, where $x$ is the accuracy\n",
    "        or \n",
    "$$\n",
    "y = ln(\\frac{\\text{number of correct}}{\\text{number of incorrect}})\n",
    "$$\n",
    "\n",
    "   **Extreme Cases**\n",
    "    1. $ln(8/0) = \\infty$ means listening to this learner and don't worry about other learners\n",
    "    2. $ln(0/8) = - \\infty$ means listening to this liar and take the opposite prediction and don't worry about other learners.\n",
    "\n",
    "   **Back to the example**\n",
    "   1. 0.84, 1.3, 1.84 weights\n",
    "   2. For positive area, add weights\n",
    "   3. For negative area, substract weights.\n",
    "   4. Sum the values for each sub-regions, positive values give positive prediction\n",
    "   \n",
    "### ADABOOS in sklearn\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "**Hyperpareameters**\n",
    "\n",
    "1. base_estimator: The model utilized for the weak learner\n",
    "\n",
    "2. n_estimators: the maximum number of weak learners used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson8: Model Evaluation Metrics (Compare algorithms)\n",
    "\n",
    "\n",
    "## How to find the models can be generalized well?\n",
    "\n",
    "**Split the data set to training set and test set**\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "\n",
    "### Medical Model\n",
    "| | Diagnosed Sick | Diagnosed Healthy|\n",
    "|:-|:-|:-|\n",
    "| Sick | True Positive  | False Negative | \n",
    "| Healthy |False Positive |True Negative |\n",
    "\n",
    "\n",
    "| | Diagnosed Sick | Diagnosed Healthy|\n",
    "|:-|:-|:-|\n",
    "| Sick | 1000  | 200 | \n",
    "| Healthy |800 |8000 |\n",
    "\n",
    "### 1. Accuracy & Precision\n",
    "\n",
    "#### **Accuracy**: $\\frac{TP+TN}{Total}$\n",
    "1. from sklearn.metrics import accuracy_score\n",
    "    accuracy_score(y_true, y_pred)\n",
    "    \n",
    "\n",
    "2. When *Accuracy* **won't work**?\n",
    "\n",
    "   When the data has a very skew distribution. It is likely to see the model with high accuracy but without classifying correctly the negative ones.\n",
    "\n",
    "#### **Precision**: $\\frac{TP}{TP + FP }$\n",
    "\n",
    "### 2. Sensitivity & Specifitiy\n",
    "\n",
    "#### **Sensitivity (Recall)**: $\\frac{TP}{TP + FN}$\n",
    "\n",
    "\n",
    "#### **Specificity**: $\\frac{TN }{ FP + TN}$\n",
    "\n",
    "\n",
    "### 3. Focus on False classifications\n",
    "\n",
    "**Precisions and Recall**\n",
    "\n",
    "| Medical| Spam Email | \n",
    "|:-|:-|\n",
    "|False Negative matters | False Positive matters|\n",
    "|Want High Recall | Want High Precision|\n",
    "\n",
    "\n",
    "\n",
    "### 4. F1 Score\n",
    "\n",
    "| | Medical| Spam Email | \n",
    "|:-|:-|:-|\n",
    "|| Recall | Precision | \n",
    "|Precision| 55.7 | 76.9| \n",
    "|Recall| 83.3 | 37%| \n",
    "\n",
    "\n",
    "**F1 score combines two scores into one**\n",
    "\n",
    "Average does not work well for lousy model, thus it is not a good idea.\n",
    "\n",
    "**Harmonic mean**\n",
    "\n",
    "$$\n",
    "F_1SCORE = \\frac{2\\cdot precision \\cdot recall}{precision + recall}\n",
    "$$\n",
    "\n",
    "### $F_\\beta$ SCORE\n",
    "\n",
    "Precision -> F_0.5 Score --> F_1 Score -> F_2 Score -> Recall\n",
    "\n",
    "$$\n",
    "F_\\beta SCORE = (1+\\beta^2)\\frac{precision \\cdot recall}{\\beta^2 \\cdot predision + recall}\n",
    "$$\n",
    "\n",
    "*Boundary of $\\beta$*\n",
    "\n",
    "1. $\\beta = 0$ --> Precision\n",
    "2. $\\beta \\rightarrow \\infty$ --> Recall\n",
    "\n",
    "## QUIZ QUESTION\n",
    "\n",
    "Out of the following three models, which one should have an F-beta score of 2, 1, and 0.5? Match each model with its corresponding score.\n",
    "\n",
    "Detecting malfunctioning parts in a spaceship\n",
    "   \n",
    "   F2\n",
    "\n",
    "Sending phone notifications about videos a user may like\n",
    "   \n",
    "   F1\n",
    "\n",
    "Sending promotional material in the mail to potential clients\n",
    "\n",
    "   F0.5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **Type I error**:  False Positive (healthy person diagnosed sick)\n",
    "\n",
    "In the spam email model, **False Positive** is worse! \n",
    "\n",
    "\n",
    "#### **Type II error**: False Negative(Sick person diagnosed healthy)\n",
    "\n",
    "In the medical model, **False Negative** is worse!\n",
    "\n",
    "### 5. Receiver Operating Characteristic (ROC) CURVE\n",
    "\n",
    "**1-Specificity/False Positive Rate/Type I error**\n",
    "\n",
    "**vs**\n",
    "\n",
    "**Sensitivity /True Positive Rate /1- TypeII error**\n",
    "\n",
    "\n",
    "Plot all the FP rate vs TP rate, the area under the curve is the ROC.\n",
    "\n",
    "1. Random split: ROC close to 0.5 \n",
    "2. Good split: ROC close to 0.8\n",
    "3. Perfect split: ROC close to 1.0\n",
    "\n",
    "*It is possible to have ROC less than 0.5, more blue points in red area and more red points in blue area*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'We have imbalanced classes, which metric do we definitely not want to use?': 'accuracy', 'We really want to make sure the positive cases are all caught even if that means we identify some negatives as positives': 'recall', 'When we identify something as positive, we want to be sure it is truly positive': 'precision', 'We care equally about identifying positive and negative cases': 'f1-score'}\n"
     ]
    }
   ],
   "source": [
    "# add the letter of the most appropriate metric to each statement\n",
    "# in the dictionary\n",
    "a = \"recall\"\n",
    "b = \"precision\"\n",
    "c = \"accuracy\"\n",
    "d = 'f1-score'\n",
    "\n",
    "\n",
    "seven_sol = {\n",
    "'We have imbalanced classes, which metric do we definitely not want to use?': c,\n",
    "'We really want to make sure the positive cases are all caught even if that means we identify some negatives as positives': a,\n",
    "'When we identify something as positive, we want to be sure it is truly positive': b,\n",
    "'We care equally about identifying positive and negative cases': d   \n",
    "}\n",
    "print(seven_sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Metrics\n",
    "\n",
    "## Mean Absolute Error, Mean Squared Error\n",
    "\n",
    "## R2 Score \n",
    "$$\n",
    "R2 = 1- \\frac{SSE}{sum of the (distance from mean y )^2}\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 9: Training and Tuning\n",
    "\n",
    "## Type of Errors\n",
    "\n",
    "### Underfitting (Error due to the bias)\n",
    "\n",
    ">Does not do well in the training set\n",
    "\n",
    "### Overfitting (Error due to the variance)\n",
    "\n",
    ">Does well in the trainning set but it tends to memorize the detail in the trainning set;\n",
    "\n",
    ">*Fails to identify the characteristics*;\n",
    "\n",
    ">Does not do well in the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Complexity Graph\n",
    "\n",
    "> x-axis: complexity of the model: linear, quadratic\n",
    "> y-axis: underfit --> overfit\n",
    "\n",
    "\n",
    "## Cross Validation\n",
    "\n",
    "Split the training data to k folds, k-1 folds as trainning data and the remaining one as testing data\n",
    "\n",
    "## Learning Curves\n",
    "\n",
    "Use Learning curve to identify the model is underfitting or overfitting\n",
    "\n",
    "x-axis is the number of points in the trainning set.\n",
    "> In the learning curve, there are trainning error and the CV error.\n",
    "\n",
    "|Fitting|Trend|Converge point|\n",
    "|:-|:-|:-|\n",
    "|Underfitting| Converge | Higher point|\n",
    "|Good | Converge | Lower point|\n",
    "|Overfitting| Not Converge | NAN|\n",
    "\n",
    "## Grid Search in sklearn\n",
    "> from sklearn.model_selection import GridSearchCV\n",
    "    parameters = {xxxxx}\n",
    "    from sklearn.metric import make_scorer\n",
    "    from sklearn.metrics import f1_score\n",
    "    scorer = make_scrorer(f1_score)\n",
    "    grid_obj = GridSearchCV(clf, parameters, scoring = scorer)\n",
    "    grid_fit = grid_obj.fit(X,y)\n",
    "    best_clf - grid_fit.best_estimator_\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
